# Data : https://drive.google.com/drive/folders/1bqfEXCkMcEJYZ3GEoi34qLzmf9nRdnSF

import torch
from torch import nn

import torchvision
from torchvision import datasets, models
import torchvision.transforms as transforms
import numpy as np
import matplotlib.pyplot as plt
transform = transforms.Compose(
    [transforms.Resize((40,40)),
     transforms.ToTensor(), # convert to 4d-tensor
     transforms.Normalize((0.5,0.5,0.5),(0.5,0.5,0.5))]
)


train_dir = '/content/drive/MyDrive/farmanimal/train'
test_dir = '/content/drive/MyDrive/farmanimal/test'


train_data = datasets.ImageFolder(root=train_dir,
                                  transform=transform)
test_data = datasets.ImageFolder(root=test_dir,
                                  transform=transform)


from google.colab import drive
drive.mount('/content/drive')


class_names = train_data.classes
Class_names


img,label=train_data[0]
Img.shape


from torch.utils.data import DataLoader


train_dataloader = DataLoader(train_data,batch_size=4, shuffle=True)


test_dataloader = DataLoader(test_data,batch_size=4, shuffle=False)


# DEFINE OUR MODEL
class CNNModel(nn.Module):
  def __init__(self):
    super(CNNModel,self).__init__()
    self.conv1 = nn.Conv2d(3,6,5)  #3=input , 6=output ,5=kernell
    self.maxpool1 = nn.MaxPool2d(kernel_size=2,stride=2)
    self.maxpool2 = nn.MaxPool2d(kernel_size=2,stride=2)
    self.conv2 = nn.Conv2d(6,16,5)
    self.conv3 = nn.Conv2d(16,20,3)
    self.fc1 = nn.Linear(5*5*20,100)
    self.fc2 = nn.Linear(100,10)
    self.relu = nn.ReLU()
    self.flatten = nn.Flatten()


  def forward(self,x):
    x = self.conv1(x)
    x = self.relu(x)
    x = self.maxpool1(x)
    x = self.conv2(x)
    x = self.relu(x)
    x = self.maxpool2(x)
    x = self.conv3(x)
    x = self.relu(x)
    x = self.flatten(x)
    x = self.fc1(x)
    x = self.relu(x)
    out = self.fc2(x)


    return out


model = CNNModel()
model.to('cuda')
!pip install torchinfo


from torchinfo import summary


summary(model= model)
# cross-entropy loss
loss_fn = nn.CrossEntropyLoss()
optimizer = torch.optim.SGD(model.parameters(),lr=0.001,momentum=0.9)


import time
from tqdm.auto import tqdm


def train_and_validate(model, loss_criterion, optimizer, train_dataloader, test_dataloader, epochs=25, device='cuda'):
    '''
    Function to train and validate
    Parameters
        :param model: Model to train and validate
        :param loss_criterion: Loss Criterion to minimize
        :param optimizer: Optimizer for computing gradients
        :param train_dataloader: DataLoader for training data
        :param test_dataloader: DataLoader for test/validation data
        :param epochs: Number of epochs (default=25)
        :param device: Device to perform computations ('cuda' or 'cpu')


    Returns
        model: Trained Model with best validation accuracy
        history: (dict object): Having training loss, accuracy and validation loss, accuracy
    '''


    start = time.time()
    history = []
    best_acc = 0.0


    for epoch in tqdm(range(epochs)):
        epoch_start = time.time()
        print("Epoch: {}/{}".format(epoch+1, epochs))


        model.train()


        train_loss = 0.0
        train_acc = 0.0


        valid_loss = 0.0
        valid_acc = 0.0


        for i, (inputs, labels) in enumerate(train_dataloader):


            inputs = inputs.to(device)
            labels = labels.to(device)


            # Clean existing gradients
            optimizer.zero_grad()


            # Forward pass - compute outputs on input data using the model
            outputs = model(inputs)


            # Compute loss
            loss = loss_criterion(outputs, labels)


            # Backpropagate the gradients
            loss.backward()


            # Update the parameters
            optimizer.step()


            # Compute the total loss for the batch and add it to train_loss
            train_loss += loss.item() * inputs.size(0)


            # Compute the accuracy
            ret, predictions = torch.max(outputs.data, 1)
            correct_counts = predictions.eq(labels.data.view_as(predictions))


            # Convert correct_counts to float and then compute the mean
            acc = torch.mean(correct_counts.type(torch.FloatTensor))


            # Compute total accuracy in the whole batch and add to train_acc
            train_acc += acc.item() * inputs.size(0)


        # Validation - No gradient tracking needed
        with torch.no_grad():


            model.eval()


            # Validation loop
            for j, (inputs, labels) in enumerate(test_dataloader):
                inputs = inputs.to(device)
                labels = labels.to(device)


                # Forward pass - compute outputs on input data using the model
                outputs = model(inputs)


                # Compute loss
                loss = loss_criterion(outputs, labels)


                # Compute the total loss for the batch and add it to valid_loss
                valid_loss += loss.item() * inputs.size(0)


                # Calculate validation accuracy
                ret, predictions = torch.max(outputs.data, 1)
                correct_counts = predictions.eq(labels.data.view_as(predictions))


                # Convert correct_counts to float and then compute the mean
                acc = torch.mean(correct_counts.type(torch.FloatTensor))


                # Compute total accuracy in the whole batch and add to valid_acc
                valid_acc += acc.item() * inputs.size(0)




        # Find average training loss and training accuracy
        avg_train_loss = train_loss / len(train_dataloader.dataset)
        avg_train_acc = train_acc / len(train_dataloader.dataset)


        # Find average validation loss and training accuracy
        avg_test_loss = valid_loss / len(test_dataloader.dataset)
        avg_test_acc = valid_acc / len(test_dataloader.dataset)


        history.append([avg_train_loss, avg_test_loss, avg_train_acc, avg_test_acc])


        epoch_end = time.time()


        print("Epoch : {:03d}, Training: Loss: {:.4f}, Accuracy: {:.4f}%, \n\t\tValidation : Loss : {:.4f}, Accuracy: {:.4f}%, Time: {:.4f}s".format(epoch, avg_train_loss, avg_train_acc * 100, avg_test_loss, avg_test_acc * 100, epoch_end - epoch_start))


        # Save if the model has best accuracy till now
        if avg_test_acc > best_acc:
            best_acc = avg_test_acc
            best_model = model
            torch.save(best_model, 'best_model.pt')


    return best_model, history


#Analyze the loss curve


def plot_loss(history):
  history = np.array(history)
  plt.plot(history[:,0:2])
  plt.legend(['Tr Loss', 'Val Loss'])
  plt.xlabel('Epoch Number')
  plt.ylabel('Loss')
  plt.ylim(0,3)
  # plt.savefig('cifar10_loss_curve.png')
  plt.show()


plot_loss(history)
def plot_accuracy(history):
  history = np.array(history)
  plt.plot(history[:,2:4])
  plt.legend(['Tr Accuracy', 'Val Accuracy'])
  plt.xlabel('Epoch Number')
  plt.ylabel('Accuracy')
  plt.ylim(0,1)
  # plt.savefig('cifar10_accuracy_curve.png')
  plt.show()


plot_accuracy(history)
from sklearn.metrics import confusion_matrix
import seaborn as sn
import pandas as pd


def plot_confusionMatrix(model, test_dataloader):


  y_pred = []
  y_true = []


  model.to('cpu')


  # iterate over test data
  for inputs, labels in test_dataloader:
          output = model(inputs) # Feed Network


          output = (torch.max(torch.exp(output), 1)[1]).data.cpu().numpy()
          y_pred.extend(output) # Save Prediction


          labels = labels.data.cpu().numpy()
          y_true.extend(labels) # Save Truth


  # Build confusion matrix
  cf_matrix = confusion_matrix(y_true, y_pred)
  df_cm = pd.DataFrame(cf_matrix/np.sum(cf_matrix) *10, index = [i for i in class_names],
                      columns = [i for i in class_names])
  plt.figure(figsize = (20,10))
  sn.heatmap(df_cm, annot=True)
  # plt.savefig('output.png')


plot_confusionMatrix(model, test_dataloader)

